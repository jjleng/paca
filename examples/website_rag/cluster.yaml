aws:
  cluster:
    name: website-rag
    region: us-west-2
    namespace: default
    nodeType: t2.medium
    minNodes: 2
    maxNodes: 4
  vectorStore:
     nodeType: t2.small
     replicas: 1
  prometheus:
    enabled: true
  tracing:
    enabled: true
  modelGroups:
    - nodeType: c7a.xlarge
      minInstances: 1
      maxInstances: 3
      name: gte-base
      runtime:
        image: ghcr.io/ggerganov/llama.cpp:server
      model:
        hfRepoId: jjleng/gte-base-gguf
        files: ["*.q4_0.gguf"]
      resourceRequest:
        cpu: 3600m
        memory: 6Gi
      autoScaleTriggers:
        - type: cpu
          metadata:
            type: Utilization
            value: "50"
    - nodeType: c7a.xlarge
      minInstances: 1
      maxInstances: 3
      name: llama2-7b
      runtime:
        image: ghcr.io/ggerganov/llama.cpp:server
      model:
        hfRepoId: TheBloke/Llama-2-7B-GGUF
        files: ["*.Q4_0.gguf"]
      resourceRequest:
        cpu: 3600m
        memory: 6Gi
      autoScaleTriggers:
        - type: cpu
          metadata:
            type: Utilization
            value: "50"
        - type: prometheus
          metadata:
            serverAddress: http://kube-prometheus-stack-prometheus.prometheus.svc.cluster.local:9090
            metricName: request_duration_90percentile
            threshold: '220000'
            query: |
              histogram_quantile(0.90, sum(rate(istio_request_duration_milliseconds_bucket{destination_service_name="llama2-7b", response_code="200"}[1m])) by (le))
