version: "1.2"
aws:
  cluster:
    name: phi3-mini-instruct-example # Use a name in lowercase letters with hyphens (kebab-case)
    region: us-west-2
    nodeType: t3a.medium
    minNodes: 2
    maxNodes: 4 # These nodes will host serverless functions and other essential loads
  prometheus:
    enabled: true # Enable metrics scraping with Prometheus
  mixedModelGroups: # A mixed model group can include both on-demand and spot nodes
    - name: phi3-mini-instruct # Specify a name for the model group
      isPublic: true # Make the model group accessible through a public endpoint
      nodeType: g5g.xlarge
      gpu: # This would enable inference on CUDA devices
        enabled: true # This model group runs on GPU-enabled instances
      baseInstances: 0 # Fail-safe instances, always run on-demand instances
      maxOnDemandInstances: 1 # Maximum number of on-demand instances, used as a fallback if spot instances are not available
      spot:
        minInstances: 1
        maxInstances: 2 # Prefer to run the inference backend on spot instances
      runtime:
        image: vllm/vllm-openai:v0.4.2 # Use vLLM backend
        env:
          - name: HF_TOKEN # Required to download model weights from a gated Hugging Face repo
            value: <REPLACE_WITH_YOUR_HF_TOKEN>
      model:
        hfRepoId: microsoft/Phi-3-mini-4k-instruct # Specify the Hugging Face model to run
        useModelStore: true # Don't save models to s3
      autoScaleTriggers:
        - type: prometheus
          metadata:
            serverAddress: http://kube-prometheus-stack-prometheus.prometheus.svc.cluster.local:9090 # Prometheus endpoint
            metricName: latency_p95
            threshold: '20000' # Set to 20s, tune as needed
            query: | # Trigger scaling if p95 latency exceeds 20s
              histogram_quantile(0.95, sum(rate(istio_request_duration_milliseconds_bucket{destination_service="phi3-mini-instruct.default.svc.cluster.local"}[5m])) by (le))
