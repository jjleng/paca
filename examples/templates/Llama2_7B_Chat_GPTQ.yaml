version: "1.2"
aws:
  cluster:
    name: llama2-7b-chat-example # Use a name in lowercase letters with hyphens (kebab-case)
    region: us-west-2
    nodeType: t3a.medium
    minNodes: 2
    maxNodes: 4 # These nodes will host serverless functions and other essential loads
  prometheus:
    enabled: true # Enable metrics scraping with Prometheus
  mixedModelGroups: # A mixed model group can include both on-demand and spot nodes
    - name: llama2-7b-chat # Specify a name for the model group
      isPublic: true # Make the model group accessible through a public endpoint
      nodeType: g4dn.xlarge
      gpu:
        enabled: true # This model group runs on GPU-enabled instances
      baseInstances: 0 # Fail-safe instances, always run on-demand instances
      maxOnDemandInstances: 1  # Maximum number of on-demand instances, used as a fallback if spot instances are not available
      spot:
        minInstances: 1
        maxInstances: 2 # Prefer to run the inference backend on spot instances
      runtime:
        image: vllm/vllm-openai:v0.4.2 # Use vLLM backend
      model:
        hfRepoId: TheBloke/Llama-2-7B-Chat-GPTQ # Specify the Hugging Face model to run
        useModelStore: false  # Don't save models to s3
      autoScaleTriggers:
        - type: prometheus
          metadata:
            serverAddress: http://kube-prometheus-stack-prometheus.prometheus.svc.cluster.local:9090 # Prometheus endpoint
            metricName: latency_p95
            threshold: '20000' # Set to 20s, tune as needed
            query: | # Trigger scaling if p95 latency exceeds 20s
              histogram_quantile(0.95, sum(rate(istio_request_duration_milliseconds_bucket{destination_service="llama2-7b-chat.default.svc.cluster.local"}[5m])) by (le))
